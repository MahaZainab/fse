{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ed4854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92bcfe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def count_tokens(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42f3bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_categories(token_counts, num_categories=4):\n",
    "    if not token_counts:\n",
    "        return []\n",
    "\n",
    "    valid_counts = [count for count in token_counts if count > 0]\n",
    "    \n",
    "    if not valid_counts:\n",
    "        return []\n",
    "    \n",
    "    min_tokens = min(valid_counts)\n",
    "    max_tokens = max(valid_counts)\n",
    "\n",
    "    if min_tokens == max_tokens:\n",
    "        return [(min_tokens, max_tokens)]\n",
    "    \n",
    "    # Use quantiles for more balanced distribution\n",
    "    try:\n",
    "        percentiles = np.linspace(0, 100, num_categories + 1)\n",
    "        thresholds = np.percentile(valid_counts, percentiles)\n",
    "        \n",
    "        # Ensure unique thresholds\n",
    "        thresholds = np.unique(thresholds)\n",
    "        \n",
    "        categories = []\n",
    "        for i in range(len(thresholds) - 1):\n",
    "            start = int(np.floor(thresholds[i]))\n",
    "            end = int(np.ceil(thresholds[i + 1]))\n",
    "            \n",
    "            # Avoid overlapping categories\n",
    "            if i > 0 and start <= categories[-1][1]:\n",
    "                start = categories[-1][1] + 1\n",
    "            \n",
    "            if start <= end:\n",
    "                categories.append((start, end))\n",
    "        \n",
    "        # If we have fewer categories than requested due to data distribution\n",
    "        # ensure the last category covers all remaining values\n",
    "        if categories and categories[-1][1] < max_tokens:\n",
    "            categories[-1] = (categories[-1][0], max_tokens)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating quantile-based categories: {e}\")\n",
    "        # Fallback to equal-width binning\n",
    "        bin_width = (max_tokens - min_tokens) / num_categories\n",
    "        categories = []\n",
    "        for i in range(num_categories):\n",
    "            start = int(min_tokens + i * bin_width)\n",
    "            end = int(min_tokens + (i + 1) * bin_width) if i < num_categories - 1 else max_tokens\n",
    "            categories.append((start, end))\n",
    "    \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b185e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_token_length(token_count, categories):\n",
    "    for start, end in categories:\n",
    "        if start <= token_count <= end:\n",
    "            if start == end:\n",
    "                return f'{start}'\n",
    "            else:\n",
    "                return f'{start}-{end}'\n",
    "    \n",
    "    # if not in any category, assign to the closest one\n",
    "    if token_count < categories[0][0]:\n",
    "        start, end = categories[0]\n",
    "        return f'{start}-{end}' if start != end else f'{start}'\n",
    "    else:\n",
    "        start, end = categories[-1]\n",
    "        return f'{start}-{end}' if start != end else f'{start}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1bc8359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, num_categories=4):\n",
    "    # First pass: calculate all token counts\n",
    "    token_counts = []\n",
    "    for item in data:\n",
    "        token_count = count_tokens(item['question']) + count_tokens(item['code'])\n",
    "        token_counts.append(token_count)\n",
    "    \n",
    "    # Create adaptive categories\n",
    "    categories = create_categories(token_counts, num_categories)\n",
    "    \n",
    "    if not categories:\n",
    "        print(\"Warning: Could not create categories from data\")\n",
    "        return {}, []\n",
    "    \n",
    "    print(f\"Created {len(categories)} adaptive categories:\")\n",
    "    for i, (start, end) in enumerate(categories):\n",
    "        if start == end:\n",
    "            print(f\"  Category {i+1}: {start} tokens\")\n",
    "        else:\n",
    "            print(f\"  Category {i+1}: {start}-{end} tokens\")\n",
    "    \n",
    "    # Second pass: assign items to categories and calculate metrics\n",
    "    for i, item in enumerate(data):\n",
    "        item['token_count'] = token_counts[i]\n",
    "        item['token_category'] = categorize_token_length(token_counts[i], categories)\n",
    "    \n",
    "    # Group by token categories\n",
    "    grouped_data = defaultdict(list)\n",
    "    for item in data:\n",
    "        grouped_data[item['token_category']].append(item)\n",
    "    \n",
    "    # Create ordered list of category names\n",
    "    category_names = []\n",
    "    for start, end in categories:\n",
    "        if start == end:\n",
    "            category_names.append(f'{start}')\n",
    "        else:\n",
    "            category_names.append(f'{start}-{end}')\n",
    "    \n",
    "    results = {}\n",
    "    for category in category_names:\n",
    "        if category in grouped_data:\n",
    "            items = grouped_data[category]\n",
    "            \n",
    "            # Filter out None values before calculating means\n",
    "            accuracy_scores = [item['accuracy']['score'] for item in items if item['accuracy']['score'] is not None]\n",
    "            completeness_scores = [item['completeness']['score'] for item in items if item['completeness']['score'] is not None]\n",
    "            relevance_scores = [item['relevance']['score'] for item in items if item['relevance']['score'] is not None]\n",
    "            clarity_scores = [item['clarity']['score'] for item in items if item['clarity']['score'] is not None]\n",
    "            \n",
    "            results[category] = {\n",
    "                'accuracy': np.mean(accuracy_scores) if accuracy_scores else 0,\n",
    "                'completeness': np.mean(completeness_scores) if completeness_scores else 0,\n",
    "                'relevance': np.mean(relevance_scores) if relevance_scores else 0,\n",
    "                'clarity': np.mean(clarity_scores) if clarity_scores else 0,\n",
    "                'count': len(items)\n",
    "            }\n",
    "        else:\n",
    "            # Category exists but has no data\n",
    "            results[category] = {\n",
    "                'accuracy': 0,\n",
    "                'completeness': 0,\n",
    "                'relevance': 0,\n",
    "                'clarity': 0,\n",
    "                'count': 0\n",
    "            }\n",
    "    \n",
    "    return results, category_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c17ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_figure_plot(processed_data, category_names, save_path=None):\n",
    "    metrics = ['accuracy', 'completeness', 'relevance', 'clarity']\n",
    "    \n",
    "    # Set up the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Use a color palette that works well with varying numbers of categories\n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(category_names)))\n",
    "    \n",
    "    # Set up bar positions\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.8 / len(category_names)  # Adjust width based on number of categories\n",
    "    \n",
    "    # Create bars for each token length category\n",
    "    for i, category in enumerate(category_names):\n",
    "        if category in processed_data:\n",
    "            scores = [\n",
    "                processed_data[category]['accuracy'],\n",
    "                processed_data[category]['completeness'],\n",
    "                processed_data[category]['relevance'],\n",
    "                processed_data[category]['clarity']\n",
    "            ]\n",
    "            \n",
    "            bars = ax.bar(x + i * width - (len(category_names) - 1) * width / 2, \n",
    "                         scores, width, \n",
    "                         label=f'{category} tokens', color=colors[i], alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Metrics', fontsize=14)\n",
    "    ax.set_ylabel('Score', fontsize=14)\n",
    "    ax.set_title(f'Performance of CodeLlama over various question token length for CodeQA', \n",
    "                fontsize=16, pad=20)\n",
    "    \n",
    "    # Set x-axis labels\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(['Accuracy', 'Completeness', 'Relevance', 'Clarity'], \n",
    "                       fontsize=16)\n",
    "    \n",
    "    # Set y-axis\n",
    "    ax.set_ylim(0, 5)\n",
    "    ax.set_yticks(np.arange(0, 6, 1))\n",
    "    ax.tick_params(axis='y', labelsize=12)\n",
    "    \n",
    "    # Add grid lines\n",
    "    ax.grid(True, axis='y', alpha=0.3, linestyle='--')\n",
    "    ax.set_axisbelow(True)\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(title='Token Length Range', title_fontsize=12, fontsize=11, \n",
    "              loc='upper left', bbox_to_anchor=(0, 1))\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c040ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_statistics(processed_data, category_names):\n",
    "    print(\"Detailed Performance Statistics by Token Length Category:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total_questions = sum(data['count'] for data in processed_data.values())\n",
    "    print(f\"\\nTotal questions analyzed: {total_questions}\")\n",
    "    \n",
    "    for category in category_names:\n",
    "        if category in processed_data:\n",
    "            data = processed_data[category]\n",
    "            if data['count'] > 0:\n",
    "                print(f\"\\nToken Length Range: {category} ({data['count']} questions)\")\n",
    "                print(f\"  Accuracy:     {data['accuracy']:.2f}\")\n",
    "                print(f\"  Completeness: {data['completeness']:.2f}\")\n",
    "                print(f\"  Relevance:    {data['relevance']:.2f}\")\n",
    "                print(f\"  Clarity:      {data['clarity']:.2f}\")\n",
    "            else:\n",
    "                print(f\"\\nToken Length Range: {category} (0 questions)\")\n",
    "                print(\"  No data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c975ad23",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'CodeLlama_CodeQA_llm_judge.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Load and process data\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCodeLlama_CodeQA_llm_judge.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#     raw_data = load_data(file_path)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#     chunk_size = 5\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#     dataset = raw_data[3*chunk_size:5*chunk_size]\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m questions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_data\u001b[39m(file_path):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      3\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CodeLlama_CodeQA_llm_judge.json'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load and process data\n",
    "    file_path = 'CodeQA_Codellama.json'\n",
    "    dataset = load_data(file_path)\n",
    "#     raw_data = load_data(file_path)\n",
    "#     chunk_size = 5\n",
    "#     dataset = raw_data[3*chunk_size:5*chunk_size]\n",
    "    \n",
    "    print(f\"Loaded {len(dataset)} questions\")\n",
    "    \n",
    "    # Process data with adaptive categories (you can change num_categories as needed)\n",
    "    processed_data, category_names = process_data(dataset, num_categories=5)\n",
    "    \n",
    "    print(f\"\\nData distribution by adaptive token length categories:\")\n",
    "    for category in category_names:\n",
    "        if category in processed_data:\n",
    "            print(f\"{category}: {processed_data[category]['count']} questions\")\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = create_figure_plot(\n",
    "        processed_data, \n",
    "        category_names, \n",
    "        save_path='RQ2_CodeQA_CodeLlama.png'\n",
    "    )\n",
    "    \n",
    "    # Display detailed statistics\n",
    "    display_statistics(processed_data, category_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ab13d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a010d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
